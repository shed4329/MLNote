# 自编码器(AE)与变分自编码器(VAE)实现与对比

## 项目简介

本项目基于TensorFlow/Keras实现了自编码器(Autoencoder, AE)和变分自编码器(Variational Autoencoder, VAE)，并在MNIST手写数字数据集上进行了训练与测试。通过该项目，你可以直观了解两种生成模型的工作原理、实现差异以及生成效果的对比。

## 环境要求

- Python 3.x
- TensorFlow 2.10.1
- NumPy 1.23.5
- Matplotlib 3.10.3

可通过以下命令安装所需依赖：
```bash
pip install tensorflow numpy matplotlib
```

## 文件说明

- `AE.py`: 自编码器的完整实现代码
- `VAE.py`: 变分自编码器的完整实现代码

## 模型架构

### 自编码器(AE)
- **编码器(Encoder)**: 通过卷积层提取输入图像特征，将28×28×1的MNIST图像压缩到32维的潜在空间
- **解码器(Decoder)**: 通过转置卷积层将32维潜在向量重建为28×28×1的图像

### 变分自编码器(VAE)
- **编码器(Encoder)**: 与AE结构类似，但输出潜在变量的均值(z_mean)和方差(z_log_var)
- **采样层(Sampling)**: 基于重参数化技巧从潜在分布中采样
- **解码器(Decoder)**: 与AE的解码器结构相同，将采样得到的潜在向量重建为图像

## 训练细节

- 数据集: MNIST手写数字数据集(60,000张训练图像，10,000张测试图像)
- 超参数:
  - 潜在空间维度: 32
  - 训练轮数: 50
  - 批次大小: 128
  - 优化器: Adam
  - AE损失函数: 均方误差(MSE)
  - VAE损失函数: 重建损失(二进制交叉熵) + KL散度损失(权重1e-3)

## 运行方法

分别运行两个Python文件即可训练对应的模型并生成结果：

```bash
python AE.py
python VAE.py
```

## 输出结果

运行后将生成以下文件：

1. 模型结构文件:
   - `autoencoder_summary.txt` (AE模型结构)
   - `vae_summary.txt` (VAE模型结构)

2. 图像文件:
   - 损失曲线: `AE_loss.png` 和 `VAE_loss.png`
   - 重建结果对比: `AE_reconstruct.png` 和 `VAE_reconstruct.png`
   - 随机生成图像: `AE_generate.png` 和 `VAE_generate.png`

## 结果说明

- **重建效果**: 两种模型都能较好地重建输入图像
- **生成效果**: VAE生成的图像质量通常优于AE，因为VAE对潜在空间施加了连续分布约束，使其具有更好的生成能力和插值特性,但是好像质量都不高
- 潜在空间特性: VAE的潜在空间具有更好的连续性和可解释性，适合进行图像插值等操作

## 两种模型的主要区别

1. 潜在空间: AE的潜在空间可能不连续，而VAE通过KL散度约束使潜在空间近似标准正态分布
2. 生成能力: VAE由于对潜在空间的约束，通常具有更好的生成新样本的能力
3. 损失函数: VAE的损失函数除了重建损失外，还包含KL散度损失以约束潜在分布
4. 采样方式: VAE使用重参数化技巧进行采样，使模型可训练

通过对比两种模型的实现和结果，可以更深入理解生成模型的设计思路和性能差异。